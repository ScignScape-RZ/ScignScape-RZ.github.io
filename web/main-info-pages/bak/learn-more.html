
<html>

<head>

@@default-style
@@font-style

<style>
img.illustrative-1 {width:50vh;position:relative;left:55px;}
img.illustrative-2 {width:80vh;position:relative;left:55px;}

td.left-of-picture {position:relative;left:30px;font-size:14pt}

</style>

</head>

<body>

<!-- <div id='top-picture'>

<div id='picture-offset'>


-->

<div id='top-offset'>


@@top-links-banner


<!-- </div> <!-- picture offset -->


</div> <!-- top offset -->


<div id='main-page-title-div'>

<div class='center-wrap top-offset-5vh'>

<h1 class='main-page-title'>
*[Shape the Platform to the Data]*
</h1>

</div>

</div>


<div id='main-page-body-div'>

<p>
Current technology has many frameworks for modeling data, from markup to 
database query to *[Semantic Web]* languages: #(XML)#, #(JSON)#, 
#(RDF)#, #(SQL)#, #(SPARQL)#.  Despite this range of options, 
developers and enterprises often feel constrained by a need to 
constantly rewrite the structure of information between its most 
natural visual and conceptual form, from users' and developers' 
perspectives, to whatever form is required by their software stack.  
Relational databases force information spaces into a tabular, 
spreadsheet-like representation --- which is an unnatural 
structure in many cases, as evidenced by the popularity 
and buzz around #(NoSQL)# engines.  On the other hand, 
those engines --- whether *[Document-Based]*, *[Big Column]*, 
*[Graph-Based]*, or *[Key-Value]* --- plant their own 
flag in the sand, and require data spaces to be conceptualized 
around their particular model.  The most flexible database 
engines are those that combine several of these models, as in a 
*[Semantic Data Lake]* --- for example, supporting both document-style 
storage (such as persistent #(XML)#) and graph-style 
(as in Semantic-Web-ready data).  These hybrids can be more 
complex to implement, however, both for providers and for 
programmers who have to decide which piece of information 
should associate with which architectural model, and how to 
merge material stored according to different models.  
</p>


<p>
The technological impasse here reflects the limitations of trying 
to think about data models in isolation from the application that 
use them.  Database programmers envision a database as something 
that stands apart from its software clients, because most 
large-scale database engines are <i>sui generis</i>, running as 
their own Operating System process with their own memory, often 
distributed over multiple computers or file systems.  
But these are implementation details driven by the requirements 
of storing large quantities of information and allowing 
concurrent access by multiple clients.  In its theory, 
a database is best considered to be an organic extension of 
an application, allowing the application to store some 
information for future reference --- information that 
will still be there if the application shuts down and later restarts.  
A database, in short, provides applications with persistent 
memory.  On the other hand, applications need to store 
data selectively and in a structure that can be recreated at 
future times: a lot of the data generated by any software
component at any one time is transient and ceases to be 
meaningful when the application stops running.  
For example, any data which depends on memory access or 
*[pointers]* becomes invalid, unless that data is converted 
to a persistent structure and then recreated during a 
future software run.  *[Object-Oriented]* databases have not been 
very popular, probably because they provide too little structure 
for distinguishing between those aspects of application 
data that are transient and those that need to be persisted.
</p>

<p>
So, while it is true that the persistent data structures that 
evolve from an application have to be curated and modeled 
within the application, they are still best considered an 
offshoot of application-specific data, not a foreign structure 
that can force applications to accept unnatural and suboptimal 
ways of representing data.  The most important criterion for a database 
engine is how seamlessly it integrates with the applications that 
use it.  There should be few steps needed to shape application 
data into persistable form and then submit it to the database, 
and then to query the database and shape the results back into  
a structure that the application can work with directly.  
This process should not require the use of a separate query 
language, the creation of query files or character strings 
expressing queries, or an interpreter to execute such queries.  
There should typically be a one-to-one correspondence between 
datatypes recognized by the application and types recognized 
by the database (perhaps as distinct tables, or as values for a classifying 
column, or as root element annotations, depending on the 
genre of database used).  That is, programmers should be aware of 
which application data types are stored as a unit in the database, 
and be aware that for each value of these types there is a 
straightforward interface for saving that value in persistent storage.  
On the other hand, the semantic models --- the meanings 
associated with data structures, in that the structures 
are given their assigned form so as to represent conceptual 
structures that reflect users' tasks and expectations --- these 
models should be derived from the operational, application side, 
not from the database side.  
</p>

<a name="mvc"></a>
<h2>Limitations of the Conventional Database-Integration Architectures</h2>

<table>
<tr>
<td class='left-of-picture'>
In conventional #(MVC)# (Model-View-Controller) Archictecture, 
data models are based on database schema, and single 
instances of a data model are produced by database queries.  
The view is a user-visible summary of the data model, expressed, 
for example, in a web page.  The controller selects which 
database entries --- which instances of data models --- to 
show to users based on their cues.  The content seen by 
users is tightly coupled to the database.  This limits 
programmers' options for conceiving datatypes in terms of 
their intrinsic semantics, as opposed to their database 
representations.  The true semantics of a datatype are 
the concepts expressed through its instances, and the 
protocols governing how these instances are processed.  
These semantics are distinct from those used by a database, 
which is focused on internal storage details, and also 
distinct from any particuar *[view]*, such as a web page, 
into which parts of a data structure are woven.
</td>
<td>
<img src='/pics/mvc.png' class='illustrative-1' alt='loading...'/>
</td>
</tr>



<tr>
<td class='left-of-picture'>
<br>
<b>By contrast</b>, in our alternative #(SMV)# (Semantic Model/View) Archictecture, 
the fundamental data model is constructed according to semantic principles:  
this means that the model is organized around language and concepts, rather than 
merely replicating database schemata or the layout of run-time software memory.  
As such, it captures conceptual and behavioral expectations of data instances 
reflected in Natural Language and in Human-Computer Interaction *[best practices]*.  
Semantic Modeling represents both how users think about different kinds 
of information, and also the protocols which ensure that data is 
used and shared properly and securely.  The Semantic Model 
is translated into multiple representations suited for different roles, 
including but not limited to persistence in a database.  The Semantic Model 
acts as an intermediary between database layers and user-visible content, 
breaking the rigid connection between these layers to ensure a more flexible 
and reusable solution.  Data is translated into multiple formats 
for use by different kinds of networks, including APIs and peer-to-peer 
networking, as well as by human viewers.    
</td>
<td>
<img src='/pics/smv.png' class='illustrative-2' alt='loading...'/>
</td>
</tr>
</table>

<a name="criteria"></a>
<h2>Crucial Component Criteria</h2>
<p>
For any problem, there are usually many possible solutions: in the technology 
realm, a programmer will usually find many different libraries, 
composed in different languages according to different philosophies, 
that provide the software capabilities or foundation allowing developers 
to meet their goals.  However, the space of solutions to choose 
from narrows down once developers start to impose criteria based 
on which design principles promote clarity, rigor, and reusability 
for the present and future of the project.  In general, there 
are three key criteria to bear in mind when evaluating 
possible components, libraries, or products:
</p>

<ul>

<li>
Native-Compiled: Components that run as native software, or can be linked 
as binaries into other native applications, have the best performance 
and are most responsive to users because they work directly with the 
native Operating System and compile to the fastest, most optimized 
code.  This is not only a question of raw speed (because of 
*[Just in Time]* compilation, software that is targed to 
virtual machines --- such as the #(JVM)#, or Java Virtual Machine ---  
can achieve execution speeds comparable to compiled #(C)# code).  
It is also a question of interoperability: software built against 
ecosystems based on a virtual runtime (like #(JVM)# or Microsoft #(.Net)#) 
tends to depend on many other elements in that ecosystem, so they cannot 
be encapsulated into distinct binary libraries that can be used by 
other applications, except insofar as those accept the overarching 
influence of the same ecosystem.  In other words, components built 
in Java or #(C#)# (or languages which use their #(VM)#s) are typically 
very difficult to use except by other components written in compatible 
languages.  By contrast, components written in #(C)# or #(C++)# can 
be binary-compiled and used equally by #(C)#-based, #(VM)#-based, 
and scripting languages.  So #(C)# and #(C++)# solutions 
--- or potentially those written in other langauges which compile to 
native machine code, like #(Lisp)# and #(Haskell)# --- 
have a wider range of environments where they can be linked or 
embedded, and therefore greater opportunities for reuse and adaptation.
</li>

<li>
Cross-Platform: As much as possible, components should be useful on 
multiple platforms and multiple computing environments.  Developers 
should prefer to incorporate solutions which can be reused 
irregardless of what kind of computer end-users have, rather than 
components which only work on Windows or on a Mac.  Web-based solutions 
are cross-platform by default (since every platform has web browsers), 
but they fail to meet the Native-Compiled criterion.  
On the other hand, developers seeking a more powerful foundation than 
web applications often turn to platform-specific frameworks, so 
they may build a front-end that runs as a Windows desktop 
program but cannot be used on Unix-based systems, or vice-versa.  
Some of the most popular #(GUI)# libraries are platform-specific, 
such as Microsoft Foundation Classes (#(MFC)#) or #[Java Swing]# 
(treating Java as a *[platform]*).  #(MFC)#, and Cocoa 
(for Apple computers), are developed by the same companies that 
create their associated Operating Systems, and these companies 
try to make #(GUI)# development more convenient --- but only 
for those writing programs solely for use on their own system.  
By contrast, frameworks like #(Qt)# and #(wxWidgets)# show that one 
can achieve professional-quality #(GUI)# front-ends in a 
fully cross-platform manner.  
</li>

<li>
Extensible: A cross-platform application can be compiled and <i>run</i> 
on many different computers, but it is equally important that an 
application can be <i>built</i> and <i>extended</i> in many 
different environments.  In other words, it should not be difficult 
for programmers (at least if they have sufficient skills to build their 
own applications) to also build their own add-ons to pre-existing 
applications, and to modify and restructure applications 
according to the needs of their own projects: adding menu items, 
customized context menus, new dialog classes, new #(GUI)# controls, 
specialized network protocols, and participation in their own data-sharing 
networks.  Ideally, an application should build from sources 
without onerous external dependencies and without 
a complex build process or having to manually edit build scripts.  
An application should not rely on developers using an 
automated packaging tool (like #[apt-get]#) to obtain 
dependencies and should not rely on dependencies being 
installed in default system locations (except for the most 
basic and common foundations used by practically every #(C)#-based software) 
--- because during development it is often crucial to 
*[sandbox]* dependencies for each project so they do not conflict 
with one another, since multiple libraries (particularly multiple 
versions of the same library) can easily conflict with each other, 
potentially rendering the computer unusable.  A good benchmark 
for *[extensibility]* is whether an application can be built and 
then run solely from an Integrated Development Environment 
(#(IDE)#) like #(Qt)# Creator, and whether the project 
distribution includes build files specific to #(IDE)#, like 
#(Qt)# project files.  Projects which <i>rely</i> on 
Autoconfigure or CMake tools (as opposed to <i>allowing</i> 
these tools to be used for convenience) could very well be 
designed in a manner that can complicate the process 
of building the application from source, which limits 
its extensibility and adaptability.
</li>

</ul>

<p>
Once you factor in these criteria, the set of best libraries to 
address a particular software problem can shrink considerably.  
The goal of #(NDP)# is to ferret out those libraries which 
pass the test and focus on ensuring that they are 
straightforward to compile, link, and use within #(NDP)# applications.
</p>


</div> <!-- main page div -->
</body>
</html>


